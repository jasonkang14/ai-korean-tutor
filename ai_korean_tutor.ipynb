{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI and Naver Clova API Setup\n",
    "\n",
    "This code block is part of the setup process for an AI Korean tutor project, which utilizes both the OpenAI API for chat completion and the Naver Clova API for speech-to-text (STT) and text-to-speech (TTS) functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from openai import OpenAI  # Import OpenAI library\n",
    "from dotenv import load_dotenv, find_dotenv  # Import dotenv for environment variable management\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "# This enables the script to access sensitive information (like API keys) securely.\n",
    "_ = load_dotenv(find_dotenv()) \n",
    "\n",
    "# Specify the GPT model to be used for the OpenAI API\n",
    "gpt_model_name = \"gpt-3.5-turbo-1106\"  # This sets the GPT model name for OpenAI's Chat Completion.\n",
    "\n",
    "# Load environment variables for Naver Cloud\n",
    "# These variables are required to authenticate and interact with Naver Cloud's APIs.\n",
    "ncloud_client_id = os.environ['NCLOUD_CLIENT_ID']  # Retrieves Naver Cloud client ID from environment variables\n",
    "ncloud_client_secret = os.environ['NCLOUD_CLIENT_SECRET']  # Retrieves Naver Cloud client secret\n",
    "\n",
    "# Initialize the OpenAI client with the API key\n",
    "# This creates a client object for interacting with OpenAI's API.\n",
    "client = OpenAI(\n",
    "    api_key=os.environ['OPENAI_API_KEY'],  # Retrieves API key from environment variables for OpenAI\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "- **Import Libraries**: The script begins by importing required libraries. `os` is used for operating system dependent functionality like reading environment variables. The `openai` library provides the Python interface to interact with OpenAI's API, and `dotenv` is used for loading environment variables from a `.env` file, which is a standard way to manage configuration settings securely.\n",
    "\n",
    "- **Loading Environment Variables**: The script uses `dotenv` to load environment variables. This approach is typically used to keep sensitive data (like API keys) out of the source code for security reasons.\n",
    "\n",
    "- **GPT Model Specification**: The `gpt_model_name` variable specifies which GPT model to use from OpenAI's offerings. In this case, it's set to `\"gpt-3.5-turbo-1106\"`, indicating the specific version of the model.\n",
    "\n",
    "- **Naver Cloud Credentials**: The script retrieves the Naver Cloud's client ID and secret from environment variables. These credentials are necessary to authenticate requests made to the Naver Clova API.\n",
    "\n",
    "- **OpenAI Client Initialization**: Finally, an instance of the `OpenAI` client is initialized with the API key from the environment variables. This client object will be used to make requests to the OpenAI API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naver Cloud Speech Recognition Integration\n",
    "\n",
    "This code block is designed to interact with Naver Cloud's Speech Recognition API, specifically for converting speech to text (`STT`). This functionality is a crucial component of the AI Korean tutor project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the requests library for making HTTP requests\n",
    "import requests\n",
    "\n",
    "# Define a function to get the transcript of an audio file\n",
    "def get_transcript(file_path):\n",
    "    # Construct the URL for the Naver Cloud Speech Recognition API\n",
    "    url = \"https://naveropenapi.apigw.ntruss.com/recog/v1/stt?lang=Kor\"\n",
    "\n",
    "    # Open the audio file in binary mode\n",
    "    data = open(file_path, 'rb')\n",
    "\n",
    "    # Define the headers for the API request\n",
    "    headers = {\n",
    "        \"X-NCP-APIGW-API-KEY-ID\": ncloud_client_id,  # Naver Cloud client ID\n",
    "        \"X-NCP-APIGW-API-KEY\": ncloud_client_secret,  # Naver Cloud client secret\n",
    "        \"Content-Type\": \"application/octet-stream\"  # Content type for audio file\n",
    "    }\n",
    "\n",
    "    # Make a POST request to the Naver Cloud API\n",
    "    response = requests.post(url, data=data, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if(response.status_code == 200):\n",
    "        return response.json()['text']  # Return the transcribed text\n",
    "    else:\n",
    "        print(\"Error : \" + response.text)  # Print the error if the request failed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "- **Function Definition**: The `get_transcript` function is defined to handle the STT process. It takes `file_path` as an input, which is the path to the audio file to be transcribed.\n",
    "\n",
    "- **API URL and Headers**: The URL for the Naver Cloud STT API is constructed, and the necessary headers are defined. These headers include the API keys for authentication and the content type for the data being sent.\n",
    "\n",
    "- **Making the API Request**: The script uses `requests.post` to send the audio file to the Naver Cloud API for transcription. The audio file is opened in binary mode and sent as data in the POST request.\n",
    "\n",
    "- **Response Handling**: The script checks the response status code. If the request was successful, it prints and returns the transcribed text from the response. If there's an error, it prints the error message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Generate Tutor's Response Using GPT\n",
    "\n",
    "This function, `get_gpt_response`, generates a response from the AI English tutor based on the student's transcribed speech and the conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined prompt that sets the context for the AI's role\n",
    "system_prompt = \"\"\"\n",
    "You are an experienced Korean tutor who graduated from Seoul National University.\n",
    "You are talking to a student who wants to practice speaking Korean. \n",
    "Help them practice speaking Korean by talking to your student and \n",
    "try to teach your student how to say what they would like to say.\n",
    "The answer must be formatted as a JSON string\n",
    "\"\"\"\n",
    "\n",
    "def get_gpt_response(transcript, history):\n",
    "    # Format the system message for context setting\n",
    "    system_message = {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": system_prompt.replace(\"\\n\", \" \")  # Removes newline characters for formatting\n",
    "    }\n",
    "    \n",
    "    # Prepare the message list combining the system message and conversation history\n",
    "    message_list = [system_message]\n",
    "    message_list.extend(history)\n",
    "    message_list.append({\"role\": \"user\", \"content\": transcript})  # Add the latest user input\n",
    "\n",
    "    # Get the AI response using the OpenAI Chat Completion API\n",
    "    response = client.chat.completions.create(\n",
    "        model=gpt_model_name,  # Specifies the GPT model to use\n",
    "        response_format={ \"type\": \"json_object\" },  # Requests response in JSON format\n",
    "        messages=message_list  # Provides the context and conversation history\n",
    "    )\n",
    "    \n",
    "    # Return the AI's message content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "\n",
    "- **System Prompt**: This sets the context for the AI, defining its role as an English tutor. The prompt is crucial as it guides the AI's responses.\n",
    "\n",
    "- **Function Parameters**: `transcript` is the latest user input (student's speech), and `history` contains previous messages in the conversation.\n",
    "\n",
    "- **Message Formatting**: The conversation history and new user input are formatted as a list of messages, each with a role (`system` or `user`) and content.\n",
    "\n",
    "- **AI Response Generation**: The `client.chat.completions.create` method is used to generate a response from the AI based on the provided context and conversation history.\n",
    "\n",
    "- **Response Handling**: The function extracts and returns the content of the AI's response, formatted as requested in JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naver Cloud Text-to-Speech (TTS) Integration\n",
    "\n",
    "This code block integrates the Naver Cloud TTS API, which is used to convert text into speech. It's particularly useful in the AI Korean tutor project for generating audio from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Define a function to convert text to speech\n",
    "def get_audio(text):\n",
    "    # URL-encode the input text\n",
    "    encoded_text = urllib.parse.quote(text)\n",
    "    # Prepare the data with parameters for the TTS request\n",
    "    tts_parameters = \"speaker=nara&volume=0&speed=0&pitch=0&format=wav&text=\" + encoded_text\n",
    "    # API URL\n",
    "    tts_url = \"https://naveropenapi.apigw.ntruss.com/tts-premium/v1/tts\"\n",
    "    \n",
    "    # Create a request object with the API URL\n",
    "    request = urllib.request.Request(tts_url)\n",
    "    # Add headers for authentication\n",
    "    request.add_header(\"X-NCP-APIGW-API-KEY-ID\", ncloud_client_id)\n",
    "    request.add_header(\"X-NCP-APIGW-API-KEY\", ncloud_client_secret)\n",
    "    \n",
    "    # Try-except block for error handling\n",
    "    try:\n",
    "        # Send the request to the Naver TTS API\n",
    "        response = urllib.request.urlopen(request, data=tts_parameters.encode('utf-8'))\n",
    "        rescode = response.getcode()\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if rescode == 200:\n",
    "            print(\"Saving TTS WAV file...\")\n",
    "            response_body = response.read()\n",
    "            file_path = 'speech.wav'\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(response_body)\n",
    "            return file_path\n",
    "        else:\n",
    "            print(f\"Error Code: {rescode}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "- **Function Definition**: `get_audio` function is defined to convert given text to speech using the Naver Cloud TTS API.\n",
    "\n",
    "- **Preparing Data**: The input text is URL-encoded, and TTS parameters (like speaker, volume, speed, pitch, format) are appended to the text. These parameters can be adjusted as per requirement.\n",
    "\n",
    "- **API Request Setup**: The request object is created with the TTS API URL, and headers for authentication are added.\n",
    "\n",
    "- **Error Handling**: The `try-except` block is used for robust error handling. It attempts to send the request and process the response, catching any exceptions that occur during the process.\n",
    "\n",
    "- **Response Processing**: If the response code is 200 (success), the script saves the response (audio data) as a WAV file. Otherwise, it prints the error code. The error handling is also improved to catch and report any exceptions that might occur during the request or file writing process.\n",
    "\n",
    "- **Saving the Audio File**: The received audio data is written to a file named speech.wav. This file path can be modified as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Playback with PyAudio\n",
    "\n",
    "This code block demonstrates how to play an audio file (specifically a WAV file) using the PyAudio library. This functionality can be essential in applications like an AI Korean tutor, where audio feedback or instructions are necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import os\n",
    "    \n",
    "def play_audio(wav_file):\n",
    "\n",
    "    # Open the .wav file\n",
    "    wf = wave.open(wav_file, 'rb')\n",
    "\n",
    "    # Create a PyAudio object\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    # Open a stream with the correct settings for the .wav file\n",
    "    stream = p.open(format=p.get_format_from_width(wf.getsampwidth()),\n",
    "                    channels=wf.getnchannels(),\n",
    "                    rate=wf.getframerate(),\n",
    "                    output=True)\n",
    "\n",
    "    # Read and play the audio in chunks\n",
    "    chunk_size = 1024\n",
    "    data = wf.readframes(chunk_size)\n",
    "    while data:\n",
    "        stream.write(data)\n",
    "        data = wf.readframes(chunk_size)\n",
    "\n",
    "    # Close the stream and PyAudio object\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    os.remove(wav_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "- **Function Definition**: The `play_audio` function is defined to handle audio playback. It takes a path to a WAV file as its argument.\n",
    "\n",
    "- **Opening the WAV File**: The WAV file is opened in read-binary mode (`'rb'`), which allows the wave module to read the audio data.\n",
    "\n",
    "- **PyAudio Object**: A PyAudio object (`p`) is created to manage audio streams.\n",
    "\n",
    "- **Audio Stream Creation**: An audio stream is opened with settings that match the audio file (format, channels, and frame rate). These settings are derived from the WAV file itself.\n",
    "\n",
    "- **Chunked Audio Playback**: The audio file is read and played in chunks (each of size 1024 bytes in this case). This approach is efficient for playing larger files, as it doesn't require loading the entire file into memory.\n",
    "\n",
    "- **Stream Management**: After the entire file is played, the audio stream and the PyAudio object are properly closed and terminated to free up resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function to Facilitate Conversation with the AI Tutor\n",
    "\n",
    "The function `talk_to_gpt` orchestrates the process of converting user speech to text, obtaining a response from the AI tutor, and then converting this response back to speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# History list to keep track of the conversation\n",
    "history = []\n",
    "\n",
    "def talk_to_gpt(file_path):\n",
    "    # Transcribe user speech to text\n",
    "    user_transcript = get_transcript(file_path)\n",
    "\n",
    "    # Get the GPT tutor's response to the user's transcript\n",
    "    # Uses only the last 10 messages in history for context\n",
    "    gpt_response = get_gpt_response(user_transcript, history[-10:])\n",
    "    \n",
    "    # Parse the JSON-formatted response from the GPT tutor\n",
    "    gpt_response = json.loads(gpt_response)\n",
    "    gpt_response = gpt_response['response']\n",
    "    \n",
    "    # Update the conversation history with user and assistant messages\n",
    "    history.extend([\n",
    "        {\"role\": \"user\", \"content\": user_transcript}, \n",
    "        {\"role\": \"assistant\", \"content\": gpt_response}\n",
    "    ])\n",
    "\n",
    "    # Play the GPT tutor's response using TTS\n",
    "    audio_file_path = get_audio(gpt_response)\n",
    "    \n",
    "    play_audio(audio_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "- **Speech-to-Text Conversion**: The `get_transcript` function is used to convert the user's speech (from the audio file at `file_path`) into text.\n",
    "\n",
    "- **AI Response Generation**: The `get_gpt_response` function generates a response from the AI tutor based on the user's transcript and recent conversation history.\n",
    "\n",
    "- **JSON Parsing**: The response from the AI tutor, which is in JSON format, is parsed to extract the textual response.\n",
    "\n",
    "- **Conversation History Management**: The conversation history is updated with the latest user and assistant (AI tutor) messages. This history is used for context in subsequent interactions.\n",
    "\n",
    "- **Printing and TTS Playback**: The AI tutor's response is recorded to `.wav` file with `get_audio` function and then played aloud using the `play_audio` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Recording Class for User Input\n",
    "\n",
    "The `AudioRecorder` class encapsulates the functionality needed to record audio from the user, which can then be processed for speech-to-text conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "\n",
    "class AudioRecorder:\n",
    "    def __init__(self):\n",
    "        self.is_recording = False      # Flag to control recording state\n",
    "        self.audio_data = []           # List to store audio frames\n",
    "        self.fs = 44100                # Sample rate (in Hz)\n",
    "        self.channels = 1              # Number of audio channels\n",
    "\n",
    "    def start_recording(self):\n",
    "        self.is_recording = True\n",
    "        self.audio_data = []\n",
    "        # Start recording in a separate thread\n",
    "        threading.Thread(target=self.record).start()\n",
    "\n",
    "    def stop_recording(self):\n",
    "        self.is_recording = False      # Stop the recording\n",
    "\n",
    "    def record(self):\n",
    "        # Set up the audio input stream\n",
    "        with sd.InputStream(samplerate=self.fs, channels=self.channels) as stream:\n",
    "            while self.is_recording:\n",
    "                data, _ = stream.read(1024)  # Read audio data from the input stream\n",
    "                self.audio_data.append(data)  # Append data to the audio_data list\n",
    "\n",
    "    def save(self, filename='output.wav'):\n",
    "        # Save the recorded audio to a file\n",
    "        if self.audio_data:\n",
    "            wav_data = np.concatenate(self.audio_data, axis=0)  # Concatenate all audio frames\n",
    "            wavio.write(filename, wav_data, self.fs, sampwidth=2)  # Write to WAV file\n",
    "            print(\"Recording saved to\", filename)\n",
    "            return filename\n",
    "        else:\n",
    "            print(\"No recording data to save.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "- **Initialization**: Sets up initial variables like sample rate, channels, and recording state.\n",
    "\n",
    "- **Start and Stop Recording**: Methods to control the start and stop of audio recording.\n",
    "\n",
    "- **Multithreading for Recording**: Uses a separate thread to handle audio input, ensuring the main program remains responsive.\n",
    "\n",
    "- **Audio Data Collection**: Continuously reads audio data from the microphone and stores it in a list.\n",
    "\n",
    "- **Saving the Recording**: Concatenates the recorded audio frames and saves them as a WAV file. This file can then be used for further processing like speech-to-text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Interface for Audio Recording and Processing\n",
    "\n",
    "This section of the code creates an interactive interface using IPython widgets to control the audio recording and initiate conversation with the AI tutor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1ee556553a4e39924de6506c009b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Start Recording', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a92386f3ac48bc9f9bf04205c0de6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Stop Recording', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started...\n",
      "Recording stopped and saved.\n",
      "Recording saved to output.wav\n",
      "response == {\"text\":\"안녕하세요 만나서 반갑습니다\"}\n",
      "response json == {'text': '안녕하세요 만나서 반갑습니다'}\n",
      "user_transcript == 안녕하세요 만나서 반갑습니다\n",
      "안녕하세요! 저도 만나서 반가워요. 한국어 연습을 도와드릴까요?\n",
      "save TTS wav\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Initialize the audio recorder\n",
    "recorder = AudioRecorder()\n",
    "\n",
    "# Create buttons for starting and stopping the recording\n",
    "start_button = widgets.Button(description=\"Start Recording\")\n",
    "stop_button = widgets.Button(description=\"Stop Recording\")\n",
    "\n",
    "def on_start_clicked(b):\n",
    "    # Function to handle start button click\n",
    "    recorder.start_recording()  # Start recording audio\n",
    "    print(\"Recording started...\")\n",
    "\n",
    "def on_stop_clicked(b):\n",
    "    # Function to handle stop button click\n",
    "    print(\"Recording stopped and saved.\")\n",
    "    recorder.stop_recording()  # Stop recording audio\n",
    "    file_name = recorder.save()  # Save the recorded audio to a file\n",
    "    talk_to_gpt(file_name)  # Process the audio file through the AI tutor\n",
    "    os.remove(file_name)  # Remove the temporary audio file\n",
    "\n",
    "# Assign the click event handlers to the buttons\n",
    "start_button.on_click(on_start_clicked)\n",
    "stop_button.on_click(on_stop_clicked)\n",
    "\n",
    "# Display the buttons in the Jupyter Notebook interface\n",
    "display(start_button, stop_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "\n",
    "- **Button Widgets**: Two buttons are created using `ipywidgets` for starting and stopping the audio recording.\n",
    "\n",
    "- **Event Handlers**: Functions `on_start_clicked` and `on_stop_clicked` are defined to handle the respective button clicks.\n",
    "    - `on_start_clicked` starts the audio recording.\n",
    "    - `on_stop_clicked` stops the recording, saves the audio, processes it through the AI tutor (`talk_to_gpt`), and then cleans up the temporary file.\n",
    "\n",
    "- **Display Widgets**: The `display` function from `IPython.display` is used to render the buttons in the Jupyter Notebook.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
