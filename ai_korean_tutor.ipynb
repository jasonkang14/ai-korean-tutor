{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initializing the OpenAI Client\n",
    "\n",
    "This section of the code is responsible for setting up the environment and initializing the OpenAI client, which we will use to interact with various OpenAI APIs including ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "_ = load_dotenv(find_dotenv()) \n",
    "\n",
    "# Specify the GPT model to be used\n",
    "gpt_model_name = \"gpt-3.5-turbo-1106\"\n",
    "\n",
    "# Load env variables for Naver Cloud\n",
    "ncloud_client_id = os.environ['NCLOUD_CLIENT_ID']\n",
    "ncloud_client_secret = os.environ['NCLOUD_CLIENT_SECRET']\n",
    "\n",
    "# Initialize the OpenAI client with the API key\n",
    "client = OpenAI(\n",
    "    api_key=os.environ['OPENAI_API_KEY'],  # Retrieves API key from environment variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Components:\n",
    "\n",
    "- **Environment Variables**: We use `dotenv` to load environment variables. This is a secure way to manage sensitive information like API keys. The `.env` file should contain your `OPENAI_API_KEY`.\n",
    "- **OpenAI Client Initialization**: We create an instance of the `OpenAI` class from the `openai` package, passing the API key from the environment variables. This client will be used to make requests to OpenAI services.\n",
    "\n",
    "> ðŸ’¡ **Tip:** Always keep your API keys secure. Never hardcode them into your scripts. Using environment variables as shown here is a best practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Function to Transcribe Audio to Text\n",
    "\n",
    "This function, `get_transcript`, takes the path of an audio file and uses OpenAI's Whisper model to transcribe the audio to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript(file_path):\n",
    "    # Open the audio file in binary read mode\n",
    "    audio_file = open(file_path, \"rb\")\n",
    "\n",
    "    # Use the OpenAI Whisper model to transcribe the audio\n",
    "    transcript = client.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",           # Specifies the Whisper model to use\n",
    "        file=audio_file,             # Passes the audio file to the API\n",
    "        response_format=\"text\"       # Requests the transcription in text format\n",
    "    )\n",
    "\n",
    "    # Return the transcription\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points:\n",
    "\n",
    "- **Opening the File**: The audio file is opened in binary read mode (`\"rb\"`), which is required for audio data processing.\n",
    "- **Transcription Request**: The `client.audio.transcriptions.create` method is used to send the audio file to OpenAI's Whisper API for transcription.\n",
    "- **Model Specification**: Here, `\"whisper-1\"` is specified as the model. Depending on your needs and OpenAI's offerings, you might use a different model version.\n",
    "- **Returning the Transcript**: The function returns the transcription result, which can then be used for further processing or displayed in the notebook.\n",
    "\n",
    "> ðŸ’¡ **Note:**  Ensure that the audio file format and content are compatible with the Whisper API's requirements for accurate transcription."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Function to Generate Tutor's Response Using GPT\n",
    "\n",
    "This function, `get_gpt_response`, generates a response from the AI English tutor based on the student's transcribed speech and the conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined prompt that sets the context for the AI's role\n",
    "system_prompt = \"\"\"\n",
    "You are an experienced English tutor who graduated from Harvard University in Boston.\n",
    "You are talking to a student who wants to practice speaking English. \n",
    "Help them practice speaking English by talking to your student and \n",
    "try to teach your student how to say what they would like to say.\n",
    "The answer must be formatted as a JSON string\n",
    "\"\"\"\n",
    "\n",
    "def get_gpt_response(transcript, history):\n",
    "    # Format the system message for context setting\n",
    "    system_message = {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": system_prompt.replace(\"\\n\", \" \")  # Removes newline characters for formatting\n",
    "    }\n",
    "    \n",
    "    # Prepare the message list combining the system message and conversation history\n",
    "    message_list = [system_message]\n",
    "    message_list.extend(history)\n",
    "    message_list.append({\"role\": \"user\", \"content\": transcript})  # Add the latest user input\n",
    "\n",
    "    # Get the AI response using the OpenAI Chat Completion API\n",
    "    response = client.chat.completions.create(\n",
    "        model=gpt_model_name,  # Specifies the GPT model to use\n",
    "        response_format={ \"type\": \"json_object\" },  # Requests response in JSON format\n",
    "        messages=message_list  # Provides the context and conversation history\n",
    "    )\n",
    "    \n",
    "    # Return the AI's message content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Components:\n",
    "\n",
    "- **System Prompt**: This sets the context for the AI, defining its role as an English tutor. The prompt is crucial as it guides the AI's responses.\n",
    "- **Function Parameters**: `transcript` is the latest user input (student's speech), and `history` contains previous messages in the conversation.\n",
    "- **Message Formatting**: The conversation history and new user input are formatted as a list of messages, each with a role (`system` or `user`) and content.\n",
    "- **AI Response Generation**: The `client.chat.completions.create` method is used to generate a response from the AI based on the provided context and conversation history.\n",
    "- **Response Handling**: The function extracts and returns the content of the AI's response, formatted as requested in JSON.\n",
    "\n",
    "> **ðŸ’¡ Tip:** This function plays a key role in maintaining the flow of conversation, ensuring that the AI's responses are contextually relevant and pedagogically sound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Function to Play AI Tutor's Response Using Text-to-Speech\n",
    "\n",
    "This function, `play_gpt_response_with_tts`, converts the AI tutor's textual response into speech using Text-to-Speech (TTS) and plays it aloud for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from playsound import playsound\n",
    "\n",
    "# Path to temporarily store the generated speech file\n",
    "speech_file_path = \"./speech.wav\"\n",
    "\n",
    "def play_gpt_response_with_tts(gpt_response):\n",
    "    # Generate speech from the GPT response using TTS\n",
    "    response = client.audio.speech.create(\n",
    "        model=\"tts-1\",          # Specifies the TTS model to use\n",
    "        voice=\"alloy\",          # Chooses a specific voice for the TTS\n",
    "        input=gpt_response      # The text input to be converted to speech\n",
    "    )\n",
    "\n",
    "    # Stream the audio to a file\n",
    "    response.stream_to_file(speech_file_path)\n",
    "\n",
    "    # Play the generated speech audio\n",
    "    playsound(speech_file_path)\n",
    "\n",
    "    # Remove the temporary speech file to clean up\n",
    "    os.remove(speech_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points:\n",
    "\n",
    "- **TTS Conversion**: The `client.audio.speech.create` method from the OpenAI API is used to convert the AI's textual response into speech. The `tts-1` model and `alloy` voice are specified here, but these can be adjusted based on your preferences.\n",
    "- **Temporary Audio File Handling**: The generated speech is streamed to a file named `speech.wav` stored at the given file path. This approach is used to handle the audio output efficiently.\n",
    "- **Audio Playback**: The `playsound` library plays the audio file, allowing the user to hear the AI's response.\n",
    "- **Cleanup**: After playing the audio, the temporary file is removed to avoid clutter and manage storage efficiently.\n",
    "\n",
    "> **ðŸ’¡ Note:** This function bridges the gap between textual AI responses and auditory output, making the interaction more engaging and accessible, especially for auditory learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Main Function to Facilitate Conversation with the AI Tutor\n",
    "\n",
    "The function `talk_to_gpt` orchestrates the process of converting user speech to text, obtaining a response from the AI tutor, and then converting this response back to speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# History list to keep track of the conversation\n",
    "history = []\n",
    "\n",
    "def talk_to_gpt(file_path):\n",
    "    # Transcribe user speech to text\n",
    "    user_transcript = get_transcript(file_path)\n",
    "\n",
    "    # Get the GPT tutor's response to the user's transcript\n",
    "    # Uses only the last 10 messages in history for context\n",
    "    gpt_response = get_gpt_response(user_transcript, history[-10:])\n",
    "    \n",
    "    # Parse the JSON-formatted response from the GPT tutor\n",
    "    gpt_response = json.loads(gpt_response)\n",
    "    gpt_response = gpt_response['response']\n",
    "    \n",
    "    # Update the conversation history with user and assistant messages\n",
    "    history.extend([\n",
    "        {\"role\": \"user\", \"content\": user_transcript}, \n",
    "        {\"role\": \"assistant\", \"content\": gpt_response}\n",
    "    ])\n",
    "\n",
    "    # Print the GPT tutor's response for logging\n",
    "    print(gpt_response)\n",
    "\n",
    "    # Play the GPT tutor's response using TTS\n",
    "    play_gpt_response_with_tts(gpt_response=gpt_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Components:\n",
    "\n",
    "- **Speech-to-Text Conversion**: The `get_transcript` function is used to convert the user's speech (from the audio file at `file_path`) into text.\n",
    "- **AI Response Generation**: The `get_gpt_response` function generates a response from the AI tutor based on the user's transcript and recent conversation history.\n",
    "- **JSON Parsing**: The response from the AI tutor, which is in JSON format, is parsed to extract the textual response.\n",
    "- **Conversation History Management**: The conversation history is updated with the latest user and assistant (AI tutor) messages. This history is used for context in subsequent interactions.\n",
    "- **Printing and TTS Playback**: The AI tutor's response is printed to the console (which can be useful for logging or debugging) and then played aloud using the `play_gpt_response_with_tts` function.\n",
    "\n",
    "> **ðŸ’¡ Note:** This function is central to the user interaction, seamlessly integrating speech-to-text, AI response generation, and text-to-speech to simulate a natural conversation flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Audio Recording Class for User Input\n",
    "\n",
    "The `AudioRecorder` class encapsulates the functionality needed to record audio from the user, which can then be processed for speech-to-text conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "\n",
    "class AudioRecorder:\n",
    "    def __init__(self):\n",
    "        self.is_recording = False      # Flag to control recording state\n",
    "        self.audio_data = []           # List to store audio frames\n",
    "        self.fs = 44100                # Sample rate (in Hz)\n",
    "        self.channels = 1              # Number of audio channels\n",
    "\n",
    "    def start_recording(self):\n",
    "        self.is_recording = True\n",
    "        self.audio_data = []\n",
    "        # Start recording in a separate thread\n",
    "        threading.Thread(target=self.record).start()\n",
    "\n",
    "    def stop_recording(self):\n",
    "        self.is_recording = False      # Stop the recording\n",
    "\n",
    "    def record(self):\n",
    "        # Set up the audio input stream\n",
    "        with sd.InputStream(samplerate=self.fs, channels=self.channels) as stream:\n",
    "            while self.is_recording:\n",
    "                data, _ = stream.read(1024)  # Read audio data from the input stream\n",
    "                self.audio_data.append(data)  # Append data to the audio_data list\n",
    "\n",
    "    def save(self, filename='output.wav'):\n",
    "        # Save the recorded audio to a file\n",
    "        if self.audio_data:\n",
    "            wav_data = np.concatenate(self.audio_data, axis=0)  # Concatenate all audio frames\n",
    "            wavio.write(filename, wav_data, self.fs, sampwidth=2)  # Write to WAV file\n",
    "            print(\"Recording saved to\", filename)\n",
    "            return filename\n",
    "        else:\n",
    "            print(\"No recording data to save.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Features:\n",
    "\n",
    "- **Initialization**: Sets up initial variables like sample rate, channels, and recording state.\n",
    "- **Start and Stop Recording**: Methods to control the start and stop of audio recording.\n",
    "- **Multithreading for Recording**: Uses a separate thread to handle audio input, ensuring the main program remains responsive.\n",
    "- **Audio Data Collection**: Continuously reads audio data from the microphone and stores it in a list.\n",
    "- **Saving the Recording**: Concatenates the recorded audio frames and saves them as a WAV file. This file can then be used for further processing like speech-to-text.\n",
    "\n",
    "> **ðŸ’¡ Note:** This class provides a foundational audio input mechanism, crucial for capturing the user's speech in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Interactive Interface for Audio Recording and Processing\n",
    "\n",
    "This section of the code creates an interactive interface using IPython widgets to control the audio recording and initiate conversation with the AI tutor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e5648c3aec49b6a3fbf3c08f95261a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Start Recording', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5dae0ba0d6440e3b433bdbc5c8f9f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Stop Recording', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started...\n",
      "Recording stopped and saved.\n",
      "Recording saved to output.wav\n",
      "Hello! I'm doing well, thank you. How about you?\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Initialize the audio recorder\n",
    "recorder = AudioRecorder()\n",
    "\n",
    "# Create buttons for starting and stopping the recording\n",
    "start_button = widgets.Button(description=\"Start Recording\")\n",
    "stop_button = widgets.Button(description=\"Stop Recording\")\n",
    "\n",
    "def on_start_clicked(b):\n",
    "    # Function to handle start button click\n",
    "    recorder.start_recording()  # Start recording audio\n",
    "    print(\"Recording started...\")\n",
    "\n",
    "def on_stop_clicked(b):\n",
    "    # Function to handle stop button click\n",
    "    print(\"Recording stopped and saved.\")\n",
    "    recorder.stop_recording()  # Stop recording audio\n",
    "    file_name = recorder.save()  # Save the recorded audio to a file\n",
    "    talk_to_gpt(file_name)  # Process the audio file through the AI tutor\n",
    "    os.remove(file_name)  # Remove the temporary audio file\n",
    "\n",
    "# Assign the click event handlers to the buttons\n",
    "start_button.on_click(on_start_clicked)\n",
    "stop_button.on_click(on_stop_clicked)\n",
    "\n",
    "# Display the buttons in the Jupyter Notebook interface\n",
    "display(start_button, stop_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Components:\n",
    "\n",
    "- **Button Widgets**: Two buttons are created using `ipywidgets` for starting and stopping the audio recording.\n",
    "- **Event Handlers**: Functions `on_start_clicked` and `on_stop_clicked` are defined to handle the respective button clicks.\n",
    "    - `on_start_clicked` starts the audio recording.\n",
    "    - `on_stop_clicked` stops the recording, saves the audio, processes it through the AI tutor (`talk_to_gpt`), and then cleans up the temporary file.\n",
    "- **Display Widgets**: The `display` function from `IPython.display` is used to render the buttons in the Jupyter Notebook.\n",
    "\n",
    "> **ðŸ’¡ Note:** This interactive setup allows users to easily control the recording process and seamlessly initiate interaction with the AI tutor, enhancing the user experience in the Jupyter Notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
